# Data split ratios
train_ratio: 0.8
val_ratio: 0.1
test_ratio: 0.1


# Training parameters
models:
  svm_1:
    tune: false
    params:
      C: 1.0
      kernel: rbf
      gamma: scale
      probability: true
      class_weight: balanced
      random_state: 0

  svm_2:
    tune: false
    params:
      C: 0.2
      probability: true
      class_weight: balanced
      random_state: 0

  svm_3:
    tune: false
    params:
      C: 1.5
      probability: true
      class_weight: balanced
      random_state: 0

  svm_4:
    tune: false
    params:
      kernel: linear
      probability: true
      class_weight: balanced
      random_state: 0

  svm_5:
    tune: false
    params:
      kernel: poly
      degree: 2
      probability: true
      class_weight: balanced
      random_state: 0

  svm_6:
    tune: false
    params:
      kernel: poly
      degree: 5
      probability: true
      class_weight: balanced
      random_state: 0

  svm_7:
    tune: false
    params:
      gamma: auto
      probability: true
      class_weight: balanced
      random_state: 0

  svm_8:
    tune: false
    params:
      gamma: scale
      probability: true
      class_weight: balanced
      random_state: 0

  svm_9:
    tune: false
    params:
      kernel: poly
      degree: 3
      gamma: auto
      probability: true
      class_weight: balanced
      random_state: 0

  svm_10:
    tune: false
    params:
      kernel: poly
      degree: 3
      gamma: scale
      probability: true
      class_weight: balanced
      random_state: 0

  svm_11:
    tune: true
    
  random_forest_1:
    tune: false
    params:
      n_estimators: 300
      max_depth: 7
      min_samples_split: 5
      min_samples_leaf: 5
      max_features: sqrt
      criterion: gini

  random_forest_2:  # zmiana: mniejsze max_depth
    tune: false
    params:
      n_estimators: 300
      max_depth: 5
      min_samples_split: 5
      min_samples_leaf: 5
      max_features: sqrt

  random_forest_3:  # zmiana: większe max_depth
    tune: false
    params:
      n_estimators: 300
      max_depth: 9
      min_samples_split: 5
      min_samples_leaf: 5
      max_features: sqrt

  random_forest_4:  # zmiana: criterion -> entropy
    tune: false
    params:
      n_estimators: 300
      max_depth: 7
      min_samples_split: 5
      min_samples_leaf: 5
      max_features: sqrt
      criterion: entropy

  random_forest_5:  # zmiana: criterion -> log_loss
    tune: false
    params:
      n_estimators: 300
      max_depth: 7
      min_samples_split: 5
      min_samples_leaf: 5
      max_features: sqrt
      criterion: log_loss


  random_forest_6:  # zmiana: mniejsze min_samples_leaf
    tune: false
    params:
      n_estimators: 300
      max_depth: 7
      min_samples_split: 5
      min_samples_leaf: 2
      max_features: sqrt

  random_forest_7:  # zmiana: większe min_samples_leaf
    tune: false
    params:
      n_estimators: 300
      max_depth: 7
      min_samples_split: 5
      min_samples_leaf: 8
      max_features: sqrt

  random_forest_8:  # zmiana: mniejsza liczba estymatorów
    tune: false
    params:
      n_estimators: 200
      max_depth: 7
      min_samples_split: 5
      min_samples_leaf: 5
      max_features: sqrt

  random_forest_9:  # zmiana: większa liczba estymatorów
    tune: false
    params:
      n_estimators: 500
      max_depth: 7
      min_samples_split: 5
      min_samples_leaf: 5
      max_features: sqrt

  random_forest_10:  # zmiana: max_features na log2
    tune: false
    params:
      n_estimators: 300
      max_depth: 7
      min_samples_split: 5
      min_samples_leaf: 5
      max_features: log2

  random_forest_11:
    tune: true

  lightgbm_1:
    tune: false
    params:
      n_estimators: 300
      num_leaves: 15
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.05
      max_depth: 4
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_2:  # zmiana: mniejsze num_leaves
    tune: false
    params:
      num_leaves: 7
      n_estimators: 300
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.05
      max_depth: 4
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_3:  # zmiana: większe num_leaves
    tune: false
    params:
      n_estimators: 300
      num_leaves: 31
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.05
      max_depth: 4
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_4:  # zmiana: większe max_depth
    tune: false
    params:
      n_estimators: 300
      num_leaves: 15
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.05
      max_depth: 6
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_5:  # zmiana: mniejsze n estimators
    tune: false
    params:
      n_estimators: 100
      num_leaves: 15
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.05
      max_depth: 4
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_6:  # zmiana: mniejszy learning_rate
    tune: false
    params:
      n_estimators: 300
      num_leaves: 15
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.01
      max_depth: 4
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_7:  # zmiana: większy learning_rate
    tune: false
    params:
      n_estimators: 300
      num_leaves: 15
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.1
      max_depth: 4
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_8:  # zmiana: mniejsze max_depth
    tune: false
    params:
      n_estimators: 300
      num_leaves: 15
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.05
      max_depth: 3
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_9:  # zmiana: większy reg_alpha
    tune: false
    params:
      n_estimators: 500
      num_leaves: 15
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.05
      max_depth: 4
      reg_alpha: 5
      reg_lambda: 5
      class_weight: balanced
      random_state: 0

  lightgbm_10:  # zmiana: większy reg_lambda
    tune: false
    params:
      n_estimators: 300
      num_leaves: 15
      min_child_samples: 100
      subsample: 0.8
      colsample_bytree: 0.8
      learning_rate: 0.05
      max_depth: 4
      reg_alpha: 5
      reg_lambda: 10
      class_weight: balanced
      random_state: 0

  lightgbm_11:
    tune: true

